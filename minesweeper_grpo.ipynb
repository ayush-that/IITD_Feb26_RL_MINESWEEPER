{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minesweeper LLM Competition - SFT + GRPO Training Pipeline\n",
    "\n",
    "## Model: Qwen2.5-14B-Instruct\n",
    "## Strategy: 3-Tier Solver -> 50K SFT Dataset -> GRPO Refinement\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load pre-generated training data (50K examples from forward-gameplay solver)\n",
    "2. SFT warmup: 1 epoch on solver-labeled optimal moves  \n",
    "3. GRPO refinement: 1200 steps with 3 reward functions (format, gameplay, strategic)\n",
    "4. Save merged model for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Environment Setup & Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ[\"VLLM_USE_TRITON_FLASH_ATTN\"] = \"0\"  # ROCm fix for Qwen2.5 SWA\nos.environ[\"HF_HUB_OFFLINE\"] = \"1\"  # Don't try to download, use local cache\n\nfrom unsloth import FastLanguageModel\nimport torch\n\nmax_seq_length = 8192  # Handle large frontier format prompts (50x50 boards can reach ~6K tokens)\nlora_rank = 64         # High rank for complex reasoning task\n\n# Use local cache path directly (HF cache is read-only)\nmodel_name = \"/root/.cache/huggingface/models--Qwen--Qwen2.5-14B-Instruct/snapshots/cf98f3b3bbb457ad9e2bb7baf9a0125b6b88caa8\"\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_name,\n    load_in_4bit=False,\n    max_seq_length=max_seq_length,\n    torch_dtype=torch.bfloat16,\n)\n\nprint(f\"Model loaded: {model.config._name_or_path}\")\nprint(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.1f}B\")\nprint(f\"Device: {model.device}\")\nprint(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Add LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=lora_rank * 2,  # alpha = 2 * rank\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "# Print trainable parameter count\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable/1e6:.1f}M / {total/1e9:.1f}B ({trainable/total*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Load Training Data & Game Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional, Set, Dict\n",
    "from datasets import Dataset, load_dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "# ================================================================\n",
    "# Game Engine (needed for reward functions)\n",
    "# ================================================================\n",
    "\n",
    "class MinesweeperGame:\n",
    "    \"\"\"Minesweeper game reconstructed from stored mine positions.\"\"\"\n",
    "\n",
    "    def __init__(self, rows, cols, mine_positions):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.mine_set = set(tuple(p) for p in mine_positions)\n",
    "        self.num_mines = len(self.mine_set)\n",
    "\n",
    "        # Calculate numbers\n",
    "        self._board = [[0] * cols for _ in range(rows)]\n",
    "        for r, c in self.mine_set:\n",
    "            self._board[r][c] = -1\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                if self._board[r][c] == -1:\n",
    "                    continue\n",
    "                count = 0\n",
    "                for dr in [-1, 0, 1]:\n",
    "                    for dc in [-1, 0, 1]:\n",
    "                        if dr == 0 and dc == 0:\n",
    "                            continue\n",
    "                        nr, nc = r + dr, c + dc\n",
    "                        if 0 <= nr < rows and 0 <= nc < cols and self._board[nr][nc] == -1:\n",
    "                            count += 1\n",
    "                self._board[r][c] = count\n",
    "\n",
    "        self.revealed = set()\n",
    "        self.flagged = set()\n",
    "        self._state = \"ongoing\"\n",
    "\n",
    "    def reveal(self, r, c):\n",
    "        \"\"\"Reveal with flood fill. Returns 'mine', 'ok', or 'win'.\"\"\"\n",
    "        if (r, c) in self.mine_set:\n",
    "            self._state = \"failed\"\n",
    "            return \"mine\"\n",
    "        stack = [(r, c)]\n",
    "        while stack:\n",
    "            cr, cc = stack.pop()\n",
    "            if (cr, cc) in self.revealed:\n",
    "                continue\n",
    "            self.revealed.add((cr, cc))\n",
    "            if self._board[cr][cc] == 0:\n",
    "                for dr in [-1, 0, 1]:\n",
    "                    for dc in [-1, 0, 1]:\n",
    "                        if dr == 0 and dc == 0:\n",
    "                            continue\n",
    "                        nr, nc = cr + dr, cc + dc\n",
    "                        if (0 <= nr < self.rows and 0 <= nc < self.cols\n",
    "                                and (nr, nc) not in self.revealed\n",
    "                                and (nr, nc) not in self.flagged):\n",
    "                            stack.append((nr, nc))\n",
    "        # Check win\n",
    "        safe_total = self.rows * self.cols - self.num_mines\n",
    "        if len(self.revealed) >= safe_total:\n",
    "            self._state = \"success\"\n",
    "            return \"win\"\n",
    "        return \"ok\"\n",
    "\n",
    "    def flag(self, r, c):\n",
    "        self.flagged.add((r, c))\n",
    "\n",
    "    def get_board(self):\n",
    "        board = [['.' for _ in range(self.cols)] for _ in range(self.rows)]\n",
    "        for r, c in self.revealed:\n",
    "            board[r][c] = str(self._board[r][c])\n",
    "        for r, c in self.flagged:\n",
    "            board[r][c] = 'F'\n",
    "        return board\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self._state\n",
    "\n",
    "\n",
    "def parse_llm_action(response):\n",
    "    \"\"\"Extract JSON action from LLM response. Returns last valid match.\"\"\"\n",
    "    best = None\n",
    "    for match in re.finditer(r'\\{[^{}]*\\}', response):\n",
    "        try:\n",
    "            action = json.loads(match.group())\n",
    "            if (\"type\" in action and \"row\" in action and \"col\" in action\n",
    "                    and action[\"type\"] in [\"reveal\", \"flag\"]):\n",
    "                action[\"row\"] = int(action[\"row\"])\n",
    "                action[\"col\"] = int(action[\"col\"])\n",
    "                best = action\n",
    "        except (json.JSONDecodeError, ValueError, TypeError):\n",
    "            continue\n",
    "    return best\n",
    "\n",
    "\n",
    "def reconstruct_game(mine_positions, rows, cols, revealed_positions, flagged_positions):\n",
    "    \"\"\"Reconstruct game state from stored data.\"\"\"\n",
    "    game = MinesweeperGame(rows, cols, mine_positions)\n",
    "    # Re-reveal all cells (with flood fill)\n",
    "    for r, c in revealed_positions:\n",
    "        if (r, c) not in game.revealed:\n",
    "            game.reveal(r, c)\n",
    "    # Re-flag\n",
    "    for r, c in flagged_positions:\n",
    "        game.flag(r, c)\n",
    "    # Reset state to ongoing (we're reconstructing mid-game)\n",
    "    game._state = \"ongoing\"\n",
    "    return game\n",
    "\n",
    "\n",
    "print(\"Game engine loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify external dependencies: solver.py and generate_data.py\nimport importlib.util, os\n\nfor module_name, filepath in [(\"solver\", \"/workspace/solver.py\"), (\"generate_data\", \"/workspace/generate_data.py\")]:\n    assert os.path.exists(filepath), f\"MISSING: {filepath}\"\n    spec = importlib.util.spec_from_file_location(module_name, filepath)\n    mod = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(mod)\n    print(f\"  {module_name}: OK ({os.path.getsize(filepath) / 1024:.1f} KB)\")\n\n# Quick solver sanity check\nfrom solver import MinesweeperSolver, solve_board\nboard = [['1','1','1'],['1','.','1'],['1','1','1']]\nsolver = solve_board(board, 3, 3, 1, full=True)\nmoves = solver.get_certain_moves()\nprint(f\"  Solver test: {len(moves)} certain moves on 3x3 with 1 mine -> {'PASS' if len(moves) == 1 else 'FAIL'}\")\nprint(\"\\nAll external files verified!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Load Pre-Generated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load pre-generated data from generate_data.py\n# FILTER: Skip 50x50 boards â€” competition max is < 50x50\ndata_file = \"minesweeper_training_data.jsonl\"\n\nraw_data = []\nskipped_50x50 = 0\nwith open(data_file, 'r') as f:\n    for line in f:\n        ex = json.loads(line.strip())\n        if ex.get('board_size') == '50x50':\n            skipped_50x50 += 1\n            continue\n        raw_data.append(ex)\n\nprint(f\"Loaded {len(raw_data)} examples (filtered {skipped_50x50} x 50x50 boards)\")\n\n# Show distribution\nstage_counts = defaultdict(int)\nsize_counts = defaultdict(int)\ndeducible_count = 0\nfor e in raw_data:\n    stage_counts[e['game_stage']] += 1\n    size_counts[e['board_size']] += 1\n    if e['is_deducible']:\n        deducible_count += 1\n\nprint(f\"\\nBoard size distribution:\")\nfor size in sorted(size_counts.keys(), key=lambda x: int(x.split('x')[0])):\n    cnt = size_counts[size]\n    print(f\"  {size}: {cnt} ({cnt/len(raw_data)*100:.1f}%)\")\n\nprint(f\"\\nGame stage distribution:\")\nfor stage in ['opening', 'early', 'mid', 'late', 'endgame', 'near_failure']:\n    cnt = stage_counts.get(stage, 0)\n    print(f\"  {stage}: {cnt} ({cnt/len(raw_data)*100:.1f}%)\")\n\nprint(f\"\\nDeducible: {deducible_count}/{len(raw_data)} ({deducible_count/len(raw_data)*100:.1f}%)\")\n\n# Show example\nex = raw_data[0]\nmsgs = json.loads(ex['messages'])\nprint(f\"\\nExample prompt (first 300 chars):\")\nprint(msgs[1]['content'][:300])\nprint(f\"\\nExample response: {msgs[2]['content']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Prepare SFT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare SFT dataset: parse messages from JSON strings to lists\n",
    "sft_items = []\n",
    "for ex in raw_data:\n",
    "    messages = json.loads(ex['messages'])  # Parse JSON string -> list of dicts\n",
    "    sft_items.append({\"messages\": messages})\n",
    "\n",
    "sft_dataset = Dataset.from_list(sft_items)\n",
    "print(f\"SFT dataset: {len(sft_dataset)} examples\")\n",
    "\n",
    "# Verify format\n",
    "assert isinstance(sft_dataset[0][\"messages\"], list), \"Messages must be a list!\"\n",
    "assert isinstance(sft_dataset[0][\"messages\"][0], dict), \"Each message must be a dict!\"\n",
    "assert \"role\" in sft_dataset[0][\"messages\"][0], \"Messages must have 'role' key!\"\n",
    "print(f\"Format check passed: messages is list of dicts with role/content\")\n",
    "\n",
    "# Verify tokenization works\n",
    "test_text = tokenizer.apply_chat_template(\n",
    "    sft_dataset[0][\"messages\"],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False,\n",
    ")\n",
    "test_tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "print(f\"Example token length: {test_tokens.input_ids.shape[1]}\")\n",
    "print(f\"First 200 chars of formatted text:\")\n",
    "print(test_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: SFT Training (Phase 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trl import SFTConfig, SFTTrainer\n\n# Formatting function required by Unsloth's SFTTrainer\n# Must always return a list of strings\ndef formatting_func(examples):\n    \"\"\"Apply chat template to convert messages to training text.\"\"\"\n    messages = examples[\"messages\"]\n    # Single example: messages is a list of dicts [{role:..., content:...}, ...]\n    # Batch: messages is a list of lists [[{role:..., content:...}, ...], ...]\n    if isinstance(messages, list) and len(messages) > 0 and isinstance(messages[0], dict):\n        # Single example - wrap in list\n        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n        return [text]\n    else:\n        # Batch\n        return [tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)\n                for msgs in messages]\n\nsft_config = SFTConfig(\n    output_dir=\"sft_checkpoint\",\n    per_device_train_batch_size=2,    # Reduced for 8192 seq length\n    gradient_accumulation_steps=8,    # Effective batch = 16\n    learning_rate=2e-5,\n    lr_scheduler_type=\"cosine\",\n    num_train_epochs=1,  # 1 epoch to avoid memorization\n    optim=\"adamw_8bit\",\n    bf16=True,\n    logging_steps=10,\n    save_steps=500,\n    save_total_limit=2,\n    max_seq_length=max_seq_length,\n    warmup_ratio=0.05,\n    report_to=\"none\",\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n)\n\nsft_trainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=sft_dataset,\n    args=sft_config,\n    formatting_func=formatting_func,\n)\n\nprint(f\"SFT config:\")\nprint(f\"  Epochs: {sft_config.num_train_epochs}\")\nprint(f\"  Batch: {sft_config.per_device_train_batch_size} x {sft_config.gradient_accumulation_steps} = {sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps}\")\nprint(f\"  LR: {sft_config.learning_rate}\")\nprint(f\"  Max seq length: {sft_config.max_seq_length}\")\nprint(f\"  Steps: ~{len(sft_dataset) // (sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps)}\")\n\nprint(\"\\nStarting SFT training...\")\nsft_trainer.train()\nprint(\"SFT training complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Save SFT Checkpoint & Quick Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SFT checkpoint\n",
    "model.save_pretrained(\"sft_checkpoint\")\n",
    "tokenizer.save_pretrained(\"sft_checkpoint\")\n",
    "print(\"SFT checkpoint saved!\")\n",
    "\n",
    "# Quick evaluation after SFT\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "FRONTIER_THRESHOLD = 16  # Must match generate_data.py and agents/minesweeper_agent.py\n",
    "\n",
    "def build_eval_prompt(board, rows, cols, mines, flags):\n",
    "    \"\"\"Build eval prompt matching training data format exactly.\"\"\"\n",
    "    mines_left = mines - flags\n",
    "    if rows <= FRONTIER_THRESHOLD and cols <= FRONTIER_THRESHOLD:\n",
    "        grid = '\\n'.join(''.join(r) for r in board)\n",
    "        return f\"MINESWEEPER {rows}x{cols} MINES:{mines} FLAGS:{flags} LEFT:{mines_left}\\n{grid}\\nRULES: .=hidden F=flag 0-8=adjacent mines\\n- If number N has N flags around it, remaining hidden neighbors are SAFE->reveal\\n- If number N needs (N-flags) more mines and has exactly that many hidden neighbors, all are MINES->flag\\n- Flag certain mines FIRST, then reveal certain safe cells\\n- NEVER act on already revealed or flagged cells\\nOutput ONLY: {{\\\"type\\\":\\\"reveal\\\"|\\\"flag\\\",\\\"row\\\":R,\\\"col\\\":C}}\"\n",
    "    else:\n",
    "        # Frontier format - matches generate_data.py and agent exactly\n",
    "        frontier_info = []\n",
    "        all_hidden_near_numbers = set()\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                if board[r][c] not in '012345678':\n",
    "                    continue\n",
    "                num = int(board[r][c])\n",
    "                fl = sum(1 for dr in [-1,0,1] for dc in [-1,0,1]\n",
    "                        if not (dr==0 and dc==0) and 0<=r+dr<rows and 0<=c+dc<cols and board[r+dr][c+dc]=='F')\n",
    "                hidden = [(r+dr,c+dc) for dr in [-1,0,1] for dc in [-1,0,1]\n",
    "                         if not (dr==0 and dc==0) and 0<=r+dr<rows and 0<=c+dc<cols and board[r+dr][c+dc]=='.']\n",
    "                if hidden:\n",
    "                    for h in hidden:\n",
    "                        all_hidden_near_numbers.add(h)\n",
    "                    hs = ''.join(f'({hr},{hc})' for hr,hc in hidden)\n",
    "                    frontier_info.append(f'R{r}C{c}={num} flags:{fl} hidden:[{hs}]')\n",
    "        \n",
    "        total_hidden = sum(1 for r in range(rows) for c in range(cols) if board[r][c] == '.')\n",
    "        interior_count = total_hidden - len(all_hidden_near_numbers)\n",
    "        frontier_str = '\\n'.join(frontier_info[:200])  # Match training data: 200 cap\n",
    "        hidden_near_str = ''.join(f'({r},{c})' for r,c in sorted(all_hidden_near_numbers)[:100])\n",
    "        \n",
    "        return f\"MINESWEEPER {rows}x{cols} MINES:{mines} FLAGS:{flags} LEFT:{mines_left}\\nFRONTIER (numbered cells with hidden neighbors):\\n{frontier_str}\\nHIDDEN NEAR NUMBERS: {hidden_near_str}\\nTOTAL HIDDEN: {total_hidden} INTERIOR(no adj number): {interior_count}\\nRULES: .=hidden F=flag 0-8=adjacent mines\\n- If number N has N flags around it, remaining hidden neighbors are SAFE->reveal\\n- If number N needs (N-flags) more mines and has exactly that many hidden neighbors, all are MINES->flag\\n- Flag certain mines FIRST, then reveal certain safe cells\\n- NEVER act on already revealed or flagged cells\\nOutput ONLY: {{\\\"type\\\":\\\"reveal\\\"|\\\"flag\\\",\\\"row\\\":R,\\\"col\\\":C}}\"\n",
    "\n",
    "\n",
    "def quick_eval(model, tokenizer, num_games=20, board_configs=None):\n",
    "    \"\"\"Quick evaluation across board sizes. Continues after invalid moves (like competition).\"\"\"\n",
    "    if board_configs is None:\n",
    "        board_configs = [\n",
    "            (6, 6, 5, 5),\n",
    "            (10, 10, 15, 5),\n",
    "            (16, 16, 40, 5),\n",
    "            (20, 20, 60, 5),\n",
    "        ]\n",
    "\n",
    "    results = {}\n",
    "    for rows, cols, mines, n_games in board_configs:\n",
    "        valid_json = 0\n",
    "        valid_moves = 0\n",
    "        invalid_moves = 0\n",
    "        total_moves = 0\n",
    "        wins = 0\n",
    "\n",
    "        for seed in range(n_games):\n",
    "            rng = random.Random(seed + 10000)\n",
    "            positions = [(r, c) for r in range(rows) for c in range(cols)]\n",
    "            mine_pos = rng.sample(positions, mines)\n",
    "            game = MinesweeperGame(rows, cols, mine_pos)\n",
    "\n",
    "            # Random first reveal\n",
    "            safe = [(r,c) for r in range(rows) for c in range(cols) if (r,c) not in game.mine_set]\n",
    "            first = rng.choice(safe)\n",
    "            game.reveal(*first)\n",
    "\n",
    "            for move_i in range(min(10, rows * cols)):\n",
    "                if game.state != \"ongoing\":\n",
    "                    if game.state == \"success\":\n",
    "                        wins += 1\n",
    "                    break\n",
    "\n",
    "                board = game.get_board()\n",
    "                flags = len(game.flagged)\n",
    "                prompt = build_eval_prompt(board, rows, cols, mines, flags)\n",
    "\n",
    "                sys_prompt = \"You are an expert Minesweeper AI. Analyze constraints and output ONLY a valid JSON action. No explanation.\"\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "                text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "                inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "                with torch.no_grad():\n",
    "                    output = model.generate(\n",
    "                        **inputs, max_new_tokens=64,\n",
    "                        temperature=1.0, do_sample=False,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                    )\n",
    "                response = tokenizer.decode(output[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "                action = parse_llm_action(response)\n",
    "\n",
    "                total_moves += 1\n",
    "                if action is not None:\n",
    "                    valid_json += 1\n",
    "                    r_act, c_act = action[\"row\"], action[\"col\"]\n",
    "                    if 0 <= r_act < rows and 0 <= c_act < cols:\n",
    "                        cell_val = board[r_act][c_act]\n",
    "                        if cell_val == '.':\n",
    "                            valid_moves += 1\n",
    "                            if action[\"type\"] == \"reveal\":\n",
    "                                game.reveal(r_act, c_act)\n",
    "                            elif action[\"type\"] == \"flag\":\n",
    "                                game.flag(r_act, c_act)\n",
    "                        else:\n",
    "                            # Invalid target (already revealed/flagged) - count penalty but continue\n",
    "                            invalid_moves += 1\n",
    "                    else:\n",
    "                        # Out of bounds - count penalty but continue\n",
    "                        invalid_moves += 1\n",
    "                else:\n",
    "                    # Invalid JSON - count penalty but continue\n",
    "                    invalid_moves += 1\n",
    "\n",
    "        json_rate = valid_json / max(total_moves, 1) * 100\n",
    "        move_rate = valid_moves / max(total_moves, 1) * 100\n",
    "        results[f\"{rows}x{cols}\"] = (json_rate, move_rate, total_moves, wins)\n",
    "        print(f\"  {rows}x{cols}: JSON={json_rate:.0f}% ValidMove={move_rate:.0f}% Wins={wins}/{n_games} Invalid={invalid_moves} ({total_moves} moves)\")\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"Post-SFT evaluation:\")\n",
    "sft_results = quick_eval(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: GRPO Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Reward Function 1: Format Reward (weight: 1.0)\n",
    "# ================================================================\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward valid JSON action format.\n",
    "    Valid JSON with correct keys -> +1.0\n",
    "    Invalid -> -3.0\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        response = completion[0][\"content\"] if isinstance(completion, list) else completion\n",
    "        action = parse_llm_action(response)\n",
    "        if action is not None:\n",
    "            scores.append(1.0)\n",
    "        else:\n",
    "            scores.append(-3.0)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Reward Function 2: Gameplay Reward (weight: 2.0)\n",
    "# ================================================================\n",
    "\n",
    "def gameplay_reward(completions, **kwargs):\n",
    "    \"\"\"Score gameplay quality by reconstructing game and simulating the action.\n",
    "\n",
    "    Uses reconstruct_game() + actual reveal/flag for accurate win detection\n",
    "    including 0-cell flood-fill cascades.\n",
    "\n",
    "    Win requires BOTH: all mines flagged AND all safe cells revealed.\n",
    "    This matches the competition specification exactly.\n",
    "\n",
    "    Scoring (raw, normalized by /25):\n",
    "    - Out of bounds:        -15 -> -0.60\n",
    "    - Already revealed:     -12 -> -0.48\n",
    "    - Already flagged:       -8 -> -0.32\n",
    "    - Flag non-mine:        -10 -> -0.40\n",
    "    - Total flags > mines:  -10 -> -0.40\n",
    "    - Reveal mine:          -25 -> -1.00\n",
    "    - Flag correct mine:    +15 -> +0.60\n",
    "    - Reveal safe (random): +10 -> +0.40\n",
    "    - Reveal safe (deducible): +15 -> +0.60\n",
    "    - Win game:            +37.5 -> +1.50 (capped)\n",
    "    \"\"\"\n",
    "    mine_positions_list = kwargs.get(\"mine_positions\", [])\n",
    "    rows_list = kwargs.get(\"rows\", [])\n",
    "    cols_list = kwargs.get(\"cols\", [])\n",
    "    num_mines_list = kwargs.get(\"num_mines\", [])\n",
    "    flagged_positions_list = kwargs.get(\"flagged_positions\", [])\n",
    "    revealed_positions_list = kwargs.get(\"revealed_positions\", [])\n",
    "    deducible_moves_list = kwargs.get(\"deducible_moves\", [])\n",
    "\n",
    "    scores = []\n",
    "    for idx, completion in enumerate(completions):\n",
    "        response = completion[0][\"content\"] if isinstance(completion, list) else completion\n",
    "        action = parse_llm_action(response)\n",
    "\n",
    "        if action is None:\n",
    "            scores.append(-10.0 / 25.0)  # Invalid format penalty\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Get stored game data\n",
    "            mine_pos = json.loads(mine_positions_list[idx]) if isinstance(mine_positions_list[idx], str) else mine_positions_list[idx]\n",
    "            rows = int(rows_list[idx])\n",
    "            cols = int(cols_list[idx])\n",
    "            num_mines = int(num_mines_list[idx])\n",
    "            flagged_pos = json.loads(flagged_positions_list[idx]) if isinstance(flagged_positions_list[idx], str) else flagged_positions_list[idx]\n",
    "            revealed_pos = json.loads(revealed_positions_list[idx]) if isinstance(revealed_positions_list[idx], str) else revealed_positions_list[idx]\n",
    "            deducible_raw = json.loads(deducible_moves_list[idx]) if isinstance(deducible_moves_list[idx], str) else deducible_moves_list[idx]\n",
    "\n",
    "            mine_set = set(tuple(p) for p in mine_pos)\n",
    "            flagged_set = set(tuple(p) for p in flagged_pos)\n",
    "            revealed_set = set(tuple(p) for p in revealed_pos)\n",
    "            deducible_set = set((m[0], m[1], m[2]) for m in deducible_raw)\n",
    "\n",
    "            row, col = action[\"row\"], action[\"col\"]\n",
    "            action_type = action[\"type\"]\n",
    "\n",
    "            # Out of bounds\n",
    "            if not (0 <= row < rows and 0 <= col < cols):\n",
    "                scores.append(-15.0 / 25.0)\n",
    "                continue\n",
    "\n",
    "            # Already revealed\n",
    "            if (row, col) in revealed_set:\n",
    "                scores.append(-12.0 / 25.0)\n",
    "                continue\n",
    "\n",
    "            # Already flagged\n",
    "            if (row, col) in flagged_set:\n",
    "                scores.append(-8.0 / 25.0)\n",
    "                continue\n",
    "\n",
    "            if action_type == \"flag\":\n",
    "                # Total flags > total mines check\n",
    "                if len(flagged_set) >= num_mines:\n",
    "                    scores.append(-10.0 / 25.0)\n",
    "                    continue\n",
    "\n",
    "                if (row, col) in mine_set:\n",
    "                    # Flag correct mine - simulate to check win\n",
    "                    game = reconstruct_game(mine_pos, rows, cols, revealed_pos, flagged_pos)\n",
    "                    game.flag(row, col)\n",
    "                    # Win: all mines flagged AND all safe revealed\n",
    "                    safe_total = rows * cols - num_mines\n",
    "                    if len(game.flagged) == num_mines and len(game.revealed) >= safe_total:\n",
    "                        scores.append(37.5 / 25.0)  # Capped win reward\n",
    "                    else:\n",
    "                        scores.append(15.0 / 25.0)\n",
    "                else:\n",
    "                    scores.append(-10.0 / 25.0)  # Flag non-mine\n",
    "\n",
    "            elif action_type == \"reveal\":\n",
    "                if (row, col) in mine_set:\n",
    "                    scores.append(-25.0 / 25.0)  # Hit mine\n",
    "                else:\n",
    "                    is_deducible = (\"reveal\", row, col) in deducible_set\n",
    "\n",
    "                    # Simulate the reveal with flood fill to detect cascade wins\n",
    "                    game = reconstruct_game(mine_pos, rows, cols, revealed_pos, flagged_pos)\n",
    "                    game.reveal(row, col)\n",
    "\n",
    "                    # Win: all safe revealed (checked by game.reveal) AND all mines flagged\n",
    "                    safe_total = rows * cols - num_mines\n",
    "                    if len(game.revealed) >= safe_total and len(game.flagged) == num_mines:\n",
    "                        scores.append(37.5 / 25.0)  # Capped win reward\n",
    "                    elif is_deducible:\n",
    "                        scores.append(15.0 / 25.0)\n",
    "                    else:\n",
    "                        scores.append(10.0 / 25.0)\n",
    "\n",
    "        except Exception as e:\n",
    "            scores.append(0.0)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Reward Function 3: Strategic Reward (weight: 0.5)\n",
    "# ================================================================\n",
    "\n",
    "def strategic_reward(completions, **kwargs):\n",
    "    \"\"\"Reward strategic play quality.\n",
    "\n",
    "    - Guessed when deducible move existed: -0.3\n",
    "    - Move adjacent to revealed numbers:  +0.2\n",
    "    - Flagged certain mine with safe reveals available: +0.15\n",
    "    - Over-flagged (flags >= mines and chose flag): -0.4\n",
    "    - Reveal triggers 0-cell cascade: +0.15\n",
    "    \"\"\"\n",
    "    deducible_moves_list = kwargs.get(\"deducible_moves\", [])\n",
    "    is_deducible_list = kwargs.get(\"is_deducible\", [])\n",
    "    mine_positions_list = kwargs.get(\"mine_positions\", [])\n",
    "    rows_list = kwargs.get(\"rows\", [])\n",
    "    cols_list = kwargs.get(\"cols\", [])\n",
    "    num_mines_list = kwargs.get(\"num_mines\", [])\n",
    "    flagged_positions_list = kwargs.get(\"flagged_positions\", [])\n",
    "    revealed_positions_list = kwargs.get(\"revealed_positions\", [])\n",
    "    board_state_list = kwargs.get(\"board_state\", [])\n",
    "\n",
    "    scores = []\n",
    "    for idx, completion in enumerate(completions):\n",
    "        response = completion[0][\"content\"] if isinstance(completion, list) else completion\n",
    "        action = parse_llm_action(response)\n",
    "\n",
    "        if action is None:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rows = int(rows_list[idx])\n",
    "            cols = int(cols_list[idx])\n",
    "            num_mines = int(num_mines_list[idx])\n",
    "            deducible_raw = json.loads(deducible_moves_list[idx]) if isinstance(deducible_moves_list[idx], str) else deducible_moves_list[idx]\n",
    "            flagged_pos = json.loads(flagged_positions_list[idx]) if isinstance(flagged_positions_list[idx], str) else flagged_positions_list[idx]\n",
    "            board = json.loads(board_state_list[idx]) if isinstance(board_state_list[idx], str) else board_state_list[idx]\n",
    "            mine_pos = json.loads(mine_positions_list[idx]) if isinstance(mine_positions_list[idx], str) else mine_positions_list[idx]\n",
    "            mine_set = set(tuple(p) for p in mine_pos)\n",
    "            flagged_set = set(tuple(p) for p in flagged_pos)\n",
    "\n",
    "            row, col = action[\"row\"], action[\"col\"]\n",
    "            action_type = action[\"type\"]\n",
    "            score = 0.0\n",
    "\n",
    "            # Check bounds\n",
    "            if not (0 <= row < rows and 0 <= col < cols):\n",
    "                scores.append(0.0)\n",
    "                continue\n",
    "\n",
    "            # Check if there were deducible moves\n",
    "            has_deducible = len(deducible_raw) > 0\n",
    "            action_is_deducible = False\n",
    "            for m in deducible_raw:\n",
    "                if m[0] == action_type and m[1] == row and m[2] == col:\n",
    "                    action_is_deducible = True\n",
    "                    break\n",
    "\n",
    "            # Penalty: guessed when deducible move existed\n",
    "            if has_deducible and not action_is_deducible:\n",
    "                score -= 0.3\n",
    "\n",
    "            # Reward: move adjacent to revealed numbers (info-gathering)\n",
    "            adjacent_to_number = False\n",
    "            for dr in [-1, 0, 1]:\n",
    "                for dc in [-1, 0, 1]:\n",
    "                    nr, nc = row + dr, col + dc\n",
    "                    if 0 <= nr < rows and 0 <= nc < cols:\n",
    "                        if board[nr][nc] in '012345678':\n",
    "                            adjacent_to_number = True\n",
    "                            break\n",
    "                if adjacent_to_number:\n",
    "                    break\n",
    "            if adjacent_to_number:\n",
    "                score += 0.2\n",
    "\n",
    "            # Reward: flagged certain mine (flag-first strategy)\n",
    "            if action_type == \"flag\" and (row, col) in mine_set and action_is_deducible:\n",
    "                score += 0.15\n",
    "\n",
    "            # Penalty: over-flagging\n",
    "            if action_type == \"flag\" and len(flagged_set) >= num_mines:\n",
    "                score -= 0.4\n",
    "\n",
    "            # Reward: reveal that triggers 0-cell cascade\n",
    "            if action_type == \"reveal\" and (row, col) not in mine_set:\n",
    "                adj_mine_count = 0\n",
    "                for dr in [-1, 0, 1]:\n",
    "                    for dc in [-1, 0, 1]:\n",
    "                        if dr == 0 and dc == 0:\n",
    "                            continue\n",
    "                        nr, nc = row + dr, col + dc\n",
    "                        if 0 <= nr < rows and 0 <= nc < cols and (nr, nc) in mine_set:\n",
    "                            adj_mine_count += 1\n",
    "                if adj_mine_count == 0:\n",
    "                    score += 0.15  # Cell is 0 -> triggers flood-fill cascade\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "        except Exception:\n",
    "            scores.append(0.0)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "print(\"Reward functions defined:\")\n",
    "print(\"  1. format_reward (weight=1.0): JSON validity\")\n",
    "print(\"  2. gameplay_reward (weight=2.0): Game rules + flood-fill win detection (requires all flags)\")\n",
    "print(\"  3. strategic_reward (weight=0.5): Strategic play + cascade detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Prepare GRPO Dataset & Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare GRPO dataset with all metadata columns\n",
    "# IMPORTANT: \"prompt\" must be a list of dicts (not JSON string) for TRL\n",
    "grpo_items = []\n",
    "skipped_long = 0\n",
    "MAX_PROMPT_TOKENS = 7500  # Leave headroom: 8192 - 128 (completion) - 564 (padding/overhead)\n",
    "\n",
    "for ex in raw_data:\n",
    "    prompt_msgs = json.loads(ex['prompt'])  # Parse JSON string -> list of dicts\n",
    "    \n",
    "    # Filter out prompts that would be truncated by TRL's max_prompt_length\n",
    "    # This prevents silent reward poisoning where model sees truncated board\n",
    "    # but reward function scores against full game state\n",
    "    text = tokenizer.apply_chat_template(prompt_msgs, tokenize=False, add_generation_prompt=True)\n",
    "    token_len = len(tokenizer(text, add_special_tokens=False).input_ids)\n",
    "    if token_len > MAX_PROMPT_TOKENS:\n",
    "        skipped_long += 1\n",
    "        continue\n",
    "    \n",
    "    item = {\n",
    "        \"prompt\": prompt_msgs,\n",
    "        \"mine_positions\": ex['mine_positions'],\n",
    "        \"rows\": ex['rows'],\n",
    "        \"cols\": ex['cols'],\n",
    "        \"num_mines\": ex['num_mines'],\n",
    "        \"flagged_positions\": ex['flagged_positions'],\n",
    "        \"revealed_positions\": ex['revealed_positions'],\n",
    "        \"board_state\": ex['board_state'],\n",
    "        \"deducible_moves\": ex['deducible_moves'],\n",
    "        \"best_move\": ex['best_move'],\n",
    "        \"is_deducible\": ex['is_deducible'],\n",
    "    }\n",
    "    grpo_items.append(item)\n",
    "\n",
    "grpo_dataset = Dataset.from_list(grpo_items)\n",
    "print(f\"GRPO dataset: {len(grpo_dataset)} examples (filtered {skipped_long} long prompts > {MAX_PROMPT_TOKENS} tokens)\")\n",
    "print(f\"Columns: {grpo_dataset.column_names}\")\n",
    "\n",
    "# Verify prompt format\n",
    "assert isinstance(grpo_dataset[0][\"prompt\"], list), \"Prompt must be a list!\"\n",
    "assert isinstance(grpo_dataset[0][\"prompt\"][0], dict), \"Each prompt msg must be a dict!\"\n",
    "print(f\"Prompt format check passed\")\n",
    "\n",
    "# Show token length distribution of remaining data\n",
    "if len(grpo_items) > 0:\n",
    "    sample_lens = []\n",
    "    for item in grpo_items[:1000]:\n",
    "        text = tokenizer.apply_chat_template(item[\"prompt\"], tokenize=False, add_generation_prompt=True)\n",
    "        sample_lens.append(len(tokenizer(text, add_special_tokens=False).input_ids))\n",
    "    print(f\"Token length stats (sample of {len(sample_lens)}): min={min(sample_lens)} max={max(sample_lens)} mean={sum(sample_lens)/len(sample_lens):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch model back to training mode\n",
    "FastLanguageModel.for_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from trl import GRPOConfig, GRPOTrainer\nfrom transformers import TrainerCallback\n\nclass CheckpointCallback(TrainerCallback):\n    \"\"\"Save ablation checkpoints at specific steps.\"\"\"\n    def __init__(self, save_steps_list):\n        self.save_steps_list = set(save_steps_list)\n\n    def on_step_end(self, args, state, control, model=None, **kwargs):\n        if state.global_step in self.save_steps_list and model is not None:\n            ckpt_dir = f\"grpo_{state.global_step}\"\n            model.save_pretrained(ckpt_dir)\n            print(f\"\\nSaved ablation checkpoint: {ckpt_dir}\")\n\ncheckpoint_cb = CheckpointCallback(save_steps_list=[400, 800])\n\ngrpo_config = GRPOConfig(\n    output_dir=\"grpo_outputs\",\n    # DAPO loss with asymmetric clipping\n    loss_type=\"dapo\",\n    epsilon=0.2,\n    epsilon_high=0.28,\n    beta=0.0,  # No KL, no ref model\n    # Generation\n    num_generations=8,\n    max_prompt_length=7500,   # Matches token filter in cell 18 - no silent truncation\n    max_completion_length=128,\n    temperature=1.0,\n    # Training\n    per_device_train_batch_size=1,   # Reduced for 8K prompts + 8 generations\n    gradient_accumulation_steps=8,   # Effective batch = 8\n    learning_rate=5e-6,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.05,\n    max_steps=1200,\n    # Optimization\n    optim=\"adamw_8bit\",\n    bf16=True,\n    # Reward\n    scale_rewards=\"batch\",\n    reward_weights=[1.0, 2.0, 0.5],\n    # Generation masking\n    mask_truncated_completions=True,\n    # Logging\n    logging_steps=5,\n    save_steps=400,\n    save_total_limit=3,\n    report_to=\"none\",\n    # Disable vLLM - Unsloth LoRA model doesn't expose vllm_engine attribute\n    use_vllm=False,\n)\n\nprint(\"GRPO config:\")\nprint(f\"  Loss: {grpo_config.loss_type}, epsilon={grpo_config.epsilon}/{grpo_config.epsilon_high}\")\nprint(f\"  Generations: {grpo_config.num_generations}\")\nprint(f\"  Batch: {grpo_config.per_device_train_batch_size} x {grpo_config.gradient_accumulation_steps}\")\nprint(f\"  Steps: {grpo_config.max_steps}\")\nprint(f\"  LR: {grpo_config.learning_rate}\")\nprint(f\"  Reward weights: {grpo_config.reward_weights}\")\nprint(f\"  vLLM: {grpo_config.use_vllm}\")\nprint(f\"  Max prompt length: {grpo_config.max_prompt_length}\")\nprint(f\"  Max completion length: {grpo_config.max_completion_length}\")\n\n# Create trainer\ngrpo_trainer = GRPOTrainer(\n    model=model,\n    processing_class=tokenizer,\n    reward_funcs=[format_reward, gameplay_reward, strategic_reward],\n    args=grpo_config,\n    train_dataset=grpo_dataset,\n    callbacks=[checkpoint_cb],\n)\n\nprint(\"\\nStarting GRPO training...\")\ntry:\n    grpo_trainer.train()\n    print(\"GRPO training complete!\")\nexcept torch.cuda.OutOfMemoryError:\n    print(\"\\nOOM with 8 generations! Falling back to num_generations=4...\")\n    torch.cuda.empty_cache()\n    # Rebuild with reduced generations\n    grpo_config_fallback = GRPOConfig(\n        output_dir=\"grpo_outputs\",\n        loss_type=\"dapo\", epsilon=0.2, epsilon_high=0.28, beta=0.0,\n        num_generations=4,  # Reduced from 8\n        max_prompt_length=7500, max_completion_length=128, temperature=1.0,\n        per_device_train_batch_size=1, gradient_accumulation_steps=8,\n        learning_rate=5e-6, lr_scheduler_type=\"cosine\", warmup_ratio=0.05,\n        max_steps=1200, optim=\"adamw_8bit\", bf16=True,\n        scale_rewards=\"batch\", reward_weights=[1.0, 2.0, 0.5],\n        mask_truncated_completions=True,\n        logging_steps=5, save_steps=400, save_total_limit=3, report_to=\"none\",\n        use_vllm=False,\n    )\n    FastLanguageModel.for_training(model)\n    grpo_trainer = GRPOTrainer(\n        model=model, processing_class=tokenizer,\n        reward_funcs=[format_reward, gameplay_reward, strategic_reward],\n        args=grpo_config_fallback, train_dataset=grpo_dataset, callbacks=[checkpoint_cb],\n    )\n    grpo_trainer.train()\n    print(\"GRPO training complete (with fallback config)!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GRPO model\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"Post-GRPO evaluation:\")\n",
    "grpo_results = quick_eval(model, tokenizer, board_configs=[\n",
    "    (6, 6, 5, 10),\n",
    "    (10, 10, 15, 10),\n",
    "    (16, 16, 40, 5),\n",
    "    (20, 20, 60, 5),\n",
    "    (30, 30, 120, 3),\n",
    "])\n",
    "\n",
    "# Compare with SFT results\n",
    "print(\"\\nComparison (JSON% / ValidMove% / Wins):\")\n",
    "for size in sorted(set(list(sft_results.keys()) + list(grpo_results.keys())), key=lambda x: int(x.split('x')[0])):\n",
    "    sft_r = sft_results.get(size, (0, 0, 0, 0))\n",
    "    grpo_r = grpo_results.get(size, (0, 0, 0, 0))\n",
    "    print(f\"  {size}: SFT={sft_r[0]:.0f}%/{sft_r[1]:.0f}%/W{sft_r[3]}  GRPO={grpo_r[0]:.0f}%/{grpo_r[1]:.0f}%/W{grpo_r[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 12: Save Final Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged model in 16-bit for evaluation\n",
    "output_path = \"/workspace/your_finetuned_model\"\n",
    "\n",
    "model.save_pretrained_merged(\n",
    "    output_path,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "\n",
    "print(f\"Model saved to: {output_path}\")\n",
    "print(\"This path is referenced in agents/minesweeper_model.py\")\n",
    "\n",
    "# Verify the saved model\n",
    "import os\n",
    "model_files = os.listdir(output_path)\n",
    "print(f\"\\nFiles: {model_files}\")\n",
    "total_size = sum(os.path.getsize(os.path.join(output_path, f)) for f in model_files if os.path.isfile(os.path.join(output_path, f)))\n",
    "print(f\"Total size: {total_size / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive final evaluation with full game playouts\n",
    "# Use higher move limit for meaningful win rate data (unlike quick_eval's 10-move sanity check)\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL COMPREHENSIVE EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def full_eval(model, tokenizer, board_configs, max_moves_per_game=500):\n",
    "    \"\"\"Full evaluation with high move limit for actual win rate measurement.\n",
    "    \n",
    "    Includes:\n",
    "    - Infinite loop detection (break after 3 identical consecutive actions)\n",
    "    - Accurate mine_hit tracking (not counted as valid_moves)\n",
    "    - Continues after invalid moves (like competition)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for rows, cols, mines, n_games in board_configs:\n",
    "        valid_json = 0\n",
    "        valid_moves = 0\n",
    "        invalid_moves = 0\n",
    "        total_moves = 0\n",
    "        wins = 0\n",
    "        mine_hits = 0\n",
    "        loops_broken = 0\n",
    "\n",
    "        for seed in range(n_games):\n",
    "            rng = random.Random(seed + 20000)\n",
    "            positions = [(r, c) for r in range(rows) for c in range(cols)]\n",
    "            mine_pos = rng.sample(positions, mines)\n",
    "            game = MinesweeperGame(rows, cols, mine_pos)\n",
    "\n",
    "            safe = [(r,c) for r in range(rows) for c in range(cols) if (r,c) not in game.mine_set]\n",
    "            first = rng.choice(safe)\n",
    "            game.reveal(*first)\n",
    "\n",
    "            last_action = None\n",
    "            repeat_count = 0\n",
    "\n",
    "            for move_i in range(max_moves_per_game):\n",
    "                if game.state != \"ongoing\":\n",
    "                    if game.state == \"success\":\n",
    "                        wins += 1\n",
    "                    break\n",
    "\n",
    "                board = game.get_board()\n",
    "                flags = len(game.flagged)\n",
    "                prompt = build_eval_prompt(board, rows, cols, mines, flags)\n",
    "\n",
    "                sys_prompt = \"You are an expert Minesweeper AI. Analyze constraints and output ONLY a valid JSON action. No explanation.\"\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "                text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "                inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "                with torch.no_grad():\n",
    "                    output = model.generate(\n",
    "                        **inputs, max_new_tokens=64,\n",
    "                        temperature=1.0, do_sample=False,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                    )\n",
    "                response = tokenizer.decode(output[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "                action = parse_llm_action(response)\n",
    "\n",
    "                # Infinite loop detection: if same action 3 times in a row, break\n",
    "                if action is not None:\n",
    "                    action_tuple = (action[\"type\"], action[\"row\"], action[\"col\"])\n",
    "                    if action_tuple == last_action:\n",
    "                        repeat_count += 1\n",
    "                        if repeat_count >= 3:\n",
    "                            loops_broken += 1\n",
    "                            break\n",
    "                    else:\n",
    "                        repeat_count = 1\n",
    "                        last_action = action_tuple\n",
    "\n",
    "                total_moves += 1\n",
    "                if action is not None:\n",
    "                    valid_json += 1\n",
    "                    r_act, c_act = action[\"row\"], action[\"col\"]\n",
    "                    if 0 <= r_act < rows and 0 <= c_act < cols:\n",
    "                        cell_val = board[r_act][c_act]\n",
    "                        if cell_val == '.':\n",
    "                            if action[\"type\"] == \"reveal\":\n",
    "                                result = game.reveal(r_act, c_act)\n",
    "                                if result == \"mine\":\n",
    "                                    mine_hits += 1\n",
    "                                    # Mine hit is NOT a \"valid move\" for metrics\n",
    "                                else:\n",
    "                                    valid_moves += 1\n",
    "                            elif action[\"type\"] == \"flag\":\n",
    "                                game.flag(r_act, c_act)\n",
    "                                valid_moves += 1\n",
    "                        else:\n",
    "                            invalid_moves += 1\n",
    "                    else:\n",
    "                        invalid_moves += 1\n",
    "                else:\n",
    "                    invalid_moves += 1\n",
    "\n",
    "        json_rate = valid_json / max(total_moves, 1) * 100\n",
    "        move_rate = valid_moves / max(total_moves, 1) * 100\n",
    "        results[f\"{rows}x{cols}\"] = (json_rate, move_rate, total_moves, wins, mine_hits, n_games)\n",
    "        loop_str = f\" Loops={loops_broken}\" if loops_broken > 0 else \"\"\n",
    "        print(f\"  {rows}x{cols}: JSON={json_rate:.0f}% ValidMove={move_rate:.0f}% Wins={wins}/{n_games} MineHits={mine_hits} Invalid={invalid_moves}{loop_str} ({total_moves} moves)\")\n",
    "\n",
    "    return results\n",
    "\n",
    "final_results = full_eval(model, tokenizer, board_configs=[\n",
    "    (6, 6, 5, 20),\n",
    "    (8, 8, 10, 20),\n",
    "    (10, 10, 15, 20),\n",
    "    (16, 16, 40, 10),\n",
    "    (20, 20, 60, 10),\n",
    "    (30, 30, 120, 5),\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for size, (json_r, move_r, total, wins, mine_hits, n_games) in sorted(final_results.items(), key=lambda x: int(x[0].split('x')[0])):\n",
    "    status = \"PASS\" if json_r >= 90 and move_r >= 50 else \"CHECK\"\n",
    "    print(f\"  [{status}] {size}: JSON={json_r:.0f}% ValidMove={move_r:.0f}% Wins={wins}/{n_games} MineHits={mine_hits} ({total} moves)\")\n",
    "\n",
    "print(\"\\nTraining pipeline complete! Model saved to /workspace/your_finetuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done! Summary of what was trained:\n",
    "#\n",
    "# Model: Qwen2.5-14B-Instruct with LoRA (rank=64, alpha=128)\n",
    "# Phase 1: SFT on 50K solver-generated examples (1 epoch)\n",
    "# Phase 2: GRPO with 3 reward functions (1200 steps, DAPO loss)\n",
    "#\n",
    "# Reward functions:\n",
    "#   1. format_reward: Valid JSON output (+1.0 / -3.0)\n",
    "#   2. gameplay_reward: Game rules scoring (normalized /25)\n",
    "#   3. strategic_reward: Strategic play quality (deducible moves, flag-first)\n",
    "#\n",
    "# Output: /workspace/your_finetuned_model (merged 16-bit)\n",
    "# Agent: /workspace/agents/ (minesweeper_model.py points to model)\n",
    "#\n",
    "# Key decisions:\n",
    "# - Compact grid prompt for <=16x16, frontier sparse for >16x16\n",
    "# - Greedy decoding at eval (temperature=0, do_sample=false)\n",
    "# - Flag-first strategy (flag certain mines before revealing safe cells)\n",
    "# - Win reward capped at +1.5 normalized to prevent gradient spikes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}